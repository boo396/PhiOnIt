TRTLLM_IMAGE=nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc6@sha256:b38228fea3dd075202053e4da05749c1711fe8dbd9b224b1f8af9264a60cc33c

# Enforce direct Docker access only (1=true, 0=false)
STRICT_DOCKER_DIRECT=0

# Hugging Face auth token (required)
HF_TOKEN=

# Model repositories (pinned to requested variants)
MODEL_REASONING_ID=nvidia/Phi-4-reasoning-plus-FP8
MODEL_MULTIMODAL_ID=nvidia/Phi-4-multimodal-instruct-NVFP4

# Public gateway and internal backend ports
PUBLIC_PORT=8080
REASONING_PORT=8355
MULTIMODAL_PORT=8356

# Container names
REASONING_CONTAINER=trtllm-phi4-reasoning
MULTIMODAL_CONTAINER=trtllm-phi4-multimodal
GATEWAY_CONTAINER=trtllm-phi4-gateway

# Optional display aliases for API callers
MODEL_REASONING_ALIAS=phi-4-reasoning-plus
MODEL_MULTIMODAL_ALIAS=phi-4-multimodal-instruct

# Host cache path for HF downloads
HF_CACHE_DIR=./data/hf-cache
